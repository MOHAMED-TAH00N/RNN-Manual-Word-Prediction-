{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0QelA8IFgqmY"
      },
      "outputs": [],
      "source": [
        "words = [\"I\", \"love\", \"deep\", \"learning\"]\n",
        "\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "X_seq = [\"I\", \"love\", \"deep\"]\n",
        "Y_target = \"learning\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_vector(size):\n",
        "    return [0.0] * size\n",
        "\n",
        "def dot(v1, v2):\n",
        "    return sum(x * y for x, y in zip(v1, v2))\n",
        "\n",
        "def add(v1, v2):\n",
        "    return [x + y for x, y in zip(v1, v2)]\n",
        "\n",
        "def tanh(v):\n",
        "    from math import tanh\n",
        "    return [tanh(x) for x in v]\n",
        "\n",
        "def softmax(v):\n",
        "    from math import exp\n",
        "    exp_v = [exp(x) for x in v]\n",
        "    total = sum(exp_v)\n",
        "    return [x / total for x in exp_v]\n"
      ],
      "metadata": {
        "id": "KvMv3GAFhLE1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "vocab_size = len(words)\n",
        "embedding_size = vocab_size\n",
        "hidden_size = 4\n",
        "output_size = vocab_size\n",
        "\n",
        "def random_matrix(rows, cols):\n",
        "    return [[random.uniform(-1, 1) for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "Wxh = random_matrix(hidden_size, vocab_size)\n",
        "Whh = random_matrix(hidden_size, hidden_size)\n",
        "Why = random_matrix(output_size, hidden_size)\n",
        "bh = zero_vector(hidden_size)\n",
        "by = zero_vector(output_size)\n"
      ],
      "metadata": {
        "id": "6lvdtY-LhRDg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(idx, size):\n",
        "    vec = [0.0] * size\n",
        "    vec[idx] = 1.0\n",
        "    return vec\n",
        "\n",
        "def forward(X_seq):\n",
        "    h_prev = zero_vector(hidden_size)\n",
        "    for word in X_seq:\n",
        "        x = one_hot(word_to_idx[word], vocab_size)\n",
        "\n",
        "\n",
        "        h_input = zero_vector(hidden_size)\n",
        "        for i in range(hidden_size):\n",
        "            h_input[i] = dot(Wxh[i], x) + dot(Whh[i], h_prev) + bh[i]\n",
        "        h_prev = tanh(h_input)\n",
        "\n",
        "    y = zero_vector(output_size)\n",
        "    for i in range(output_size):\n",
        "        y[i] = dot(Why[i], h_prev) + by[i]\n",
        "\n",
        "    probs = softmax(y)\n",
        "    return probs\n"
      ],
      "metadata": {
        "id": "VCDfVhUNhZHn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = forward(X_seq)\n",
        "\n",
        "predicted_idx = probs.index(max(probs))\n",
        "predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "print(f\"Predicted word: {predicted_word}\")\n",
        "print(f\"Target word: {Y_target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-K53Qddhc12",
        "outputId": "1c8eab7f-aa53-49b9-fffd-a99e21713172"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted word: deep\n",
            "Target word: learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(predicted_probs, target_idx):\n",
        "    from math import log\n",
        "    return -log(predicted_probs[target_idx] + 1e-9)\n"
      ],
      "metadata": {
        "id": "ZRzfKI8xhfso"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_softmax_cross_entropy(probs, target_idx):\n",
        "\n",
        "    d = [p for p in probs]\n",
        "    d[target_idx] -= 1.0\n",
        "    return d\n"
      ],
      "metadata": {
        "id": "RpiV41p9j0cT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "epochs = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    h_prev = zero_vector(hidden_size)\n",
        "    xs = []\n",
        "    hs = [h_prev]\n",
        "\n",
        "    for word in X_seq:\n",
        "        x = one_hot(word_to_idx[word], vocab_size)\n",
        "        xs.append(x)\n",
        "\n",
        "        h_input = zero_vector(hidden_size)\n",
        "        for i in range(hidden_size):\n",
        "            h_input[i] = dot(Wxh[i], x) + dot(Whh[i], h_prev) + bh[i]\n",
        "        h_prev = tanh(h_input)\n",
        "        hs.append(h_prev)\n",
        "\n",
        "\n",
        "    y = zero_vector(output_size)\n",
        "    for i in range(output_size):\n",
        "        y[i] = dot(Why[i], h_prev) + by[i]\n",
        "\n",
        "    probs = softmax(y)\n",
        "    loss = cross_entropy_loss(probs, word_to_idx[Y_target])\n",
        "\n",
        "\n",
        "    dy = d_softmax_cross_entropy(probs, word_to_idx[Y_target])\n",
        "\n",
        "    dWhy = [[0.0]*hidden_size for _ in range(output_size)]\n",
        "    dby = [0.0]*output_size\n",
        "    for i in range(output_size):\n",
        "        for j in range(hidden_size):\n",
        "            dWhy[i][j] = dy[i] * hs[-1][j]\n",
        "        dby[i] = dy[i]\n",
        "\n",
        "    dh = [0.0] * hidden_size\n",
        "    for i in range(hidden_size):\n",
        "        for j in range(output_size):\n",
        "            dh[i] += Why[j][i] * dy[j]\n",
        "\n",
        "    dtanh = [1 - h**2 for h in hs[-1]]\n",
        "    dhraw = [dh[i] * dtanh[i] for i in range(hidden_size)]\n",
        "\n",
        "    dWxh = [[0.0]*vocab_size for _ in range(hidden_size)]\n",
        "    dWhh = [[0.0]*hidden_size for _ in range(hidden_size)]\n",
        "    dbh = [0.0]*hidden_size\n",
        "\n",
        "    for i in range(hidden_size):\n",
        "        for j in range(vocab_size):\n",
        "            dWxh[i][j] = dhraw[i] * xs[-1][j]\n",
        "        for j in range(hidden_size):\n",
        "            dWhh[i][j] = dhraw[i] * hs[-2][j]\n",
        "        dbh[i] = dhraw[i]\n",
        "\n",
        "    for i in range(output_size):\n",
        "        for j in range(hidden_size):\n",
        "            Why[i][j] -= learning_rate * dWhy[i][j]\n",
        "        by[i] -= learning_rate * dby[i]\n",
        "\n",
        "    for i in range(hidden_size):\n",
        "        for j in range(vocab_size):\n",
        "            Wxh[i][j] -= learning_rate * dWxh[i][j]\n",
        "        for j in range(hidden_size):\n",
        "            Whh[i][j] -= learning_rate * dWhh[i][j]\n",
        "        bh[i] -= learning_rate * dbh[i]\n",
        "\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9O-sKMBaj2-f",
        "outputId": "0d378bf4-efd9-4c49-cfd1-1a1a577aed2f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 2.2446\n",
            "Epoch 100, Loss: 0.0229\n",
            "Epoch 200, Loss: 0.0099\n",
            "Epoch 300, Loss: 0.0062\n",
            "Epoch 400, Loss: 0.0045\n",
            "Epoch 500, Loss: 0.0035\n",
            "Epoch 600, Loss: 0.0029\n",
            "Epoch 700, Loss: 0.0024\n",
            "Epoch 800, Loss: 0.0021\n",
            "Epoch 900, Loss: 0.0018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = forward(X_seq)\n",
        "predicted_idx = probs.index(max(probs))\n",
        "predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "print(f\"\\nFinal predicted word: {predicted_word}\")\n",
        "print(f\"Target word: {Y_target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFvLxyH-j8jR",
        "outputId": "f3003874-9423-49d0-a34a-16a1c0b6f844"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final predicted word: learning\n",
            "Target word: learning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgasGBNLkDNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}